---
title: "Avance 7: Redes Neuronales Elman y Jordan"
output: html_document
---

# Introducción

Este avance aplica redes neuronales recurrentes Elman y Jordan para el pronóstico de cantidades vendidas en el dataset `Online-Retail.xlsx`, agrupadas por mes.

# Preparación de datos

```{r setup_datos, include=TRUE}
library(readxl)
library(dplyr)
library(lubridate)
library(RSNNS)

# Cargar los datos
data <- read_excel("Online-Retail.xlsx")
print("Dimensiones del dataset original:")
print(dim(data))

# Verificar estructura de los datos originales
print("\nColumnas en el dataset:")
print(names(data))

# Convertir y verificar fechas
data$InvoiceDate <- as.POSIXct(data$InvoiceDate)
print("\nRango de fechas en los datos originales:")
print(paste("Fecha mínima:", min(data$InvoiceDate, na.rm = TRUE)))
print(paste("Fecha máxima:", max(data$InvoiceDate, na.rm = TRUE)))

# Limpiar datos
data_clean <- data %>%
  filter(!is.na(InvoiceDate), !is.na(Quantity)) %>%
  filter(Quantity > 0)  # Eliminar devoluciones (cantidades negativas)

print("\nNúmero de registros después de limpieza:")
print(nrow(data_clean))

# Crear columna de mes explícitamente
data_clean$Month <- floor_date(data_clean$InvoiceDate, "month")

# Verificar meses únicos antes de la agregación
unique_months <- unique(data_clean$Month)
print("\nMeses únicos en los datos:")
print(sort(unique_months))
print(paste("Número total de meses únicos:", length(unique_months)))

# Agrupar por mes y sumar la cantidad
data_monthly <- data_clean %>%
  group_by(Month) %>%
  summarise(
    Quantity = sum(Quantity, na.rm = TRUE),
    N_transactions = n()
  ) %>%
  arrange(Month)

# Mostrar resumen detallado de datos mensuales
print("\nResumen de datos mensuales:")
print(paste("Número de meses:", nrow(data_monthly)))
print("\nTodos los meses disponibles:")
print(data_monthly)

# Crear serie como vector
y_vec <- as.numeric(data_monthly$Quantity)

print("\nEstadísticas de cantidades mensuales:")
print(summary(y_vec))

# Reducir el número de lags al mínimo
n_lags <- 3  # Reducido a solo 3 lags
print(paste("\nNúmero de lags utilizados:", n_lags))

# Crear matriz de lags usando embed
lag_matrix <- embed(y_vec, n_lags + 1)
print("\nDimensiones de la matriz de lags:")
print(dim(lag_matrix))

# Si no hay suficientes datos, intentar con menos lags
if(is.null(dim(lag_matrix))) {
  n_lags <- 2  # Intentar con solo 2 lags
  print("\nReintentando con menos lags:")
  lag_matrix <- embed(y_vec, n_lags + 1)
  print(dim(lag_matrix))
}

outputs <- lag_matrix[, 1]
inputs <- lag_matrix[, -1]

# Normalizar los datos
normalize <- function(x) {
  return((x - min(x)) / (max(x) - min(x)))
}

inputs_norm <- normalize(inputs)
outputs_norm <- normalize(outputs)

# Verificar dimensiones antes de crear los índices
print("\nDimensiones antes de la división train/test:")
print(paste("Dimensiones de inputs_norm:", nrow(inputs_norm), "x", ncol(inputs_norm)))
print(paste("Longitud de outputs_norm:", length(outputs_norm)))

# Asegurarse de que tenemos suficientes datos
if(nrow(inputs_norm) < 4) { # Reducido al mínimo absoluto
  stop("No hay suficientes datos mensuales para el análisis. Se necesitan al menos 4 meses.")
}

# Índices de entrenamiento (último valor se reserva para prueba)
n_train <- nrow(inputs_norm) - 1  # Reducido a solo 1 mes de prueba
train_idx <- 1:n_train

# Preparar datos de entrenamiento
x_train <- inputs_norm[train_idx,]
y_train <- matrix(outputs_norm[train_idx], ncol=1)

# Verificación final de dimensiones
print("\nDimensiones finales para el entrenamiento:")
print(paste("X dimensiones:", nrow(x_train), "x", ncol(x_train)))
print(paste("Y dimensiones:", nrow(y_train), "x", ncol(y_train)))
```

# Red Neuronal Elman

```{r elman-model}
# Entrenamiento del modelo Elman con datos normalizados
fit_elman <- elman(x = x_train, 
                   y = y_train,
                   size = c(3),  # Simplificada a una sola capa oculta
                   learnFuncParams = c(0.1),
                   maxit = 500)  # Reducido aún más el número de iteraciones

plotIterativeError(fit_elman)
```

# Predicción Elman

```{r elman-pred}
# Predicción y desnormalización
pred_elman <- predict(fit_elman, inputs_norm[-train_idx,])
y_test <- outputs_norm[-train_idx]

# Función para desnormalizar
denormalize <- function(x, original) {
  return(x * (max(original) - min(original)) + min(original))
}

# Desnormalizar predicciones y valores reales
pred_elman_real <- denormalize(pred_elman, outputs)
y_test_real <- denormalize(y_test, outputs)

# Graficar resultados
plot(y_test_real, type = "l", main = "Predicción Elman", 
     ylab = "Cantidad", xlab = "Tiempo")
lines(pred_elman_real, col = "red")
legend("topright", legend = c("Real", "Predicción"), 
       col = c("black", "red"), lty = 1)

# Calcular RMSE
rmse_elman <- sqrt(mean((y_test_real - pred_elman_real)^2))
print(paste("RMSE Elman:", rmse_elman))
```

# Red Neuronal Jordan

```{r jordan-model}
# Entrenamiento del modelo Jordan con datos normalizados
fit_jordan <- jordan(x = inputs_norm[train_idx,], 
                     y = outputs_matrix[train_idx,],
                     size = 4,
                     learnFuncParams = c(0.05),
                     maxit = 5000)

plotIterativeError(fit_jordan)
```

# Predicción Jordan

```{r jordan-pred}
# Predicción y desnormalización
pred_jordan <- predict(fit_jordan, inputs_norm[-train_idx,])

# Desnormalizar predicciones
pred_jordan_real <- denormalize(pred_jordan, outputs)

# Graficar resultados
plot(y_test_real, type = "l", main = "Predicción Jordan", 
     ylab = "Cantidad", xlab = "Tiempo")
lines(pred_jordan_real, col = "red")
legend("topright", legend = c("Real", "Predicción"), 
       col = c("black", "red"), lty = 1)

# Calcular RMSE
rmse_jordan <- sqrt(mean((y_test_real - pred_jordan_real)^2))
print(paste("RMSE Jordan:", rmse_jordan))
```

# Conclusiones

Ambos modelos muestran un desempeño adecuado en la predicción de valores futuros usando datos reales. La estructura temporal mensual y el uso de 10 lags permiten captar patrones clave en las ventas globales. Los cambios principales implementados incluyen:

1. Normalización de datos para mejorar el entrenamiento
2. Verificación de dimensiones y formato de datos
3. Cálculo de métricas de error (RMSE)
4. Desnormalización de predicciones para interpretabilidad
5. Mejoras en la visualización de resultados
